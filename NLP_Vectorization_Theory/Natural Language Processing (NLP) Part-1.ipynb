{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadb6899",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925c0c9",
   "metadata": {},
   "source": [
    "### Defination and need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16aa45",
   "metadata": {},
   "source": [
    "**Natural Language Processing (NLP)** is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. In text data processing, NLP plays a crucial role in understanding and extracting meaning from textual data. \n",
    "Here are some key topics and techniques in NLP related to text data processing:\n",
    "\n",
    "- **Tokenization:** Tokenization is the process of breaking down text into smaller units called tokens. Tokens can be words, sentences, or even characters, depending on the level of granularity required. Tokenization is often the first step in NLP tasks.\n",
    "\n",
    "- **Stop Word Removal:** Stop words are commonly used words that carry little or no significance in a text. Examples include \"a,\" \"an,\" \"the,\" \"is,\" and \"and.\" Removing stop words can help reduce noise and improve the processing efficiency in NLP tasks.\n",
    "\n",
    "- **Stemming and Lemmatization:** Stemming and lemmatization are techniques used to reduce words to their base or root forms. Stemming involves removing suffixes from words, while lemmatization maps words to their dictionary form (lemma). For example, stemming would convert \"running\" to \"run,\" while lemmatization would convert both \"running\" and \"ran\" to \"run.\" These techniques help in reducing the dimensionality of text data.\n",
    "\n",
    "- **Part-of-Speech (POS) Tagging:** POS tagging involves assigning grammatical tags to each word in a sentence, such as noun, verb, adjective, etc. POS tagging helps in understanding the syntactic structure of a sentence and is used in various NLP tasks like information extraction and parsing.\n",
    "\n",
    "- **Named Entity Recognition (NER):** NER is the process of identifying and classifying named entities (such as person names, organizations, locations, etc.) in text. NER is commonly used in information extraction, question answering systems, and sentiment analysis.\n",
    "\n",
    "- **Sentiment Analysis:** Sentiment analysis, also known as opinion mining, is the process of determining the sentiment expressed in a given text. It involves classifying the text as positive, negative, or neutral. Sentiment analysis finds applications in customer feedback analysis, social media monitoring, and brand reputation management.\n",
    "\n",
    "- **Text Classification:** Text classification involves assigning predefined categories or labels to text documents. It can be binary (e.g., spam/not spam) or multi-class (e.g., classifying news articles into different topics). Techniques such as machine learning algorithms, deep learning models, and feature engineering are used for text classification.\n",
    "\n",
    "- **Word Embeddings:** Word embeddings are dense vector representations of words in a high-dimensional space. They capture semantic relationships between words, enabling algorithms to better understand their meaning. Popular word embedding techniques include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "- **Document Embeddings:** Document embeddings represent entire documents as fixed-length vectors. They capture the contextual information and semantic meaning of the document. Techniques such as Doc2Vec and Universal Sentence Encoder are commonly used for document embeddings.\n",
    "\n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF):** TF-IDF is a numerical statistic that reflects the importance of a word in a document or a collection of documents. It calculates a weight for each word based on its frequency in the document and its rarity in the corpus. TF-IDF is often used as a feature representation for text classification and information retrieval tasks.\n",
    "\n",
    "- **Bag-of-Words (BoW):** BoW representation represents a text document as a collection of its words, disregarding grammar and word order. It creates a histogram of word occurrences in a document, which can be used as input for various machine learning algorithms.\n",
    "\n",
    "- **Sequence Models:** Sequence models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM), are used to model the sequential nature of text data. They are effective for tasks like text generation, machine translation, and sentiment analysis where the order of words is important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac60749",
   "metadata": {},
   "source": [
    "### Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d2b12",
   "metadata": {},
   "source": [
    "> **We have to pass the text data to the model and we will get the result but eventually model only understand the numerical values as at the backend it is mainly build with numerical equations, so we have to preprocess our text data in numerical format, techniqually it's called vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6af11d",
   "metadata": {},
   "source": [
    "Vectorization techniques in NLP are used to represent text data in a numerical form that can be understood by machine learning algorithms. Here's an explanation of some popular vectorization techniques:\n",
    "\n",
    "- **Bag-of-Words (BoW):** The Bag-of-Words model represents text as a collection of unique words, disregarding grammar and word order. It creates a histogram of word occurrences in a document or a corpus. Each document is represented by a vector, where each element corresponds to the count or presence of a word in the document. BoW is simple and effective but does not capture the semantic meaning of words.\n",
    "\n",
    "- **CountVectorizer:** CountVectorizer is a specific implementation of the BoW model in scikit-learn, a popular Python library for machine learning. It converts a collection of text documents into a matrix of token counts. Each document is represented by a sparse matrix where rows correspond to documents, columns correspond to unique words in the corpus, and the cell values represent the word counts.\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency):** TF-IDF is a numerical statistic that reflects the importance of a word in a document or a collection of documents. It combines the concepts of term frequency (TF) and inverse document frequency (IDF). The TF component measures how frequently a word appears in a document, while the IDF component penalizes words that appear frequently across documents. TF-IDF assigns higher weights to words that are more specific to a document, making it useful for information retrieval and text classification tasks.\n",
    "\n",
    "- **Word2Vec:** Word2Vec is a popular word embedding technique that represents words as dense vectors in a continuous vector space. It captures semantic relationships between words by training neural networks on large text corpora. Word2Vec models can generate word embeddings that capture syntactic and semantic information, allowing algorithms to understand the meaning and context of words. Word2Vec embeddings are often pre-trained and then used as features in downstream NLP tasks.\n",
    "\n",
    "- **GloVe (Global Vectors for Word Representation):** GloVe is another widely used word embedding technique that constructs word vectors based on the co-occurrence statistics of words in a corpus. It leverages global word-to-word co-occurrence information to generate word embeddings. GloVe embeddings capture semantic relationships and exhibit interesting linear substructures, enabling meaningful arithmetic operations on word vectors.\n",
    "\n",
    "- **FastText:** FastText is an extension of Word2Vec that incorporates subword information. Instead of treating words as the smallest unit, FastText breaks words down into n-grams (subword units), which allows it to generate representations for rare or out-of-vocabulary words. FastText embeddings are beneficial for tasks involving morphologically rich languages and capturing word similarities based on subword units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca3980",
   "metadata": {},
   "source": [
    "#### Now Seeing the Techniques one by one, practically and theorytically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bec22",
   "metadata": {},
   "source": [
    "### Bag of Words(BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4beb5a",
   "metadata": {},
   "source": [
    "**The Bag-of-Words (BoW)** technique is a fundamental method for representing text data in natural language processing (NLP). It treats a document as an unordered collection of words and creates a histogram-like representation of the words' occurrences in the document. BoW is a simple yet effective way to transform textual information into a numerical representation that can be understood by machine learning algorithms.\n",
    "\n",
    "Here's how the Bag-of-Words technique works:\n",
    "\n",
    "**1) Tokenization:** The first step is to break down the text into individual words or tokens. Tokenization involves splitting the text based on spaces, punctuation, or other criteria. For example, the sentence \"I love cats and dogs\" would be tokenized into the following words: [\"I\", \"love\", \"cats\", \"and\", \"dogs\"].\n",
    "\n",
    "**2) Vocabulary Construction:** The next step is to construct a vocabulary or a set of unique words from the corpus of documents. The vocabulary contains all the distinct words present in the collection of documents. For instance, if we have three documents with the following sentences: \"I love cats,\" \"Dogs are adorable,\" and \"Cats and dogs make great pets,\" the vocabulary would consist of the words: [\"I\", \"love\", \"cats\", \"dogs\", \"are\", \"adorable\", \"and\", \"make\", \"great\", \"pets\"].\n",
    "\n",
    "**3)Vectorization:** Once the vocabulary is constructed, each document is represented as a vector. The vector has the same length as the vocabulary, and each element corresponds to the count or presence of a word in the document. There are two common ways to perform vectorization:\n",
    "\n",
    "**a. Binary Representation:** In binary representation, each element in the vector is set to 1 if the corresponding word is present in the document, and 0 otherwise. It indicates the absence or presence of words, ignoring their frequency.\n",
    "\n",
    "**b. Count Representation:** In the count representation, each element in the vector represents the count or frequency of the corresponding word in the document. For example, if the word \"cats\" appears twice in a document, the corresponding element in the vector would be 2.\n",
    "\n",
    "**4)Document Representation:** After vectorizing each document, you end up with a matrix where rows represent documents, and columns represent words from the vocabulary. Each cell in the matrix represents the occurrence or count of a word in a particular document.\n",
    "\n",
    "The Bag-of-Words technique is a basic approach that treats words as independent entities, disregarding word order and grammar. It is suitable for tasks like text classification, information retrieval, and document clustering, where the focus is on the presence or absence of words rather than their contextual meaning.\n",
    "\n",
    "Although BoW is straightforward, it has limitations. It doesn't consider word semantics or capture the order of words in a sentence, which can be crucial for certain NLP tasks. Nevertheless, with appropriate preprocessing and in combination with other techniques, BoW can serve as a useful baseline representation for text data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5960c5",
   "metadata": {},
   "source": [
    "**Advantages of Bag-of-Words representation:**\n",
    "\n",
    "1) Simplicity: BoW is a straightforward technique that is easy to understand and implement. It treats text as an unordered collection of words, disregarding grammar and word order. This simplicity makes it a good starting point for text analysis tasks.\n",
    "\n",
    "2) Efficient: BoW representation can be computationally efficient, especially when dealing with large datasets. It only requires counting the occurrences of words, making it scalable for processing large volumes of text.\n",
    "\n",
    "3) Language Independence: BoW representation is not dependent on any specific language or domain. It can be applied to any language or domain, as long as appropriate preprocessing steps are taken.\n",
    "\n",
    "4) Versatility: BoW can be used for a variety of tasks, including text classification, sentiment analysis, information retrieval, and clustering. It serves as a basic feature representation that can be fed into machine learning algorithms.\n",
    "\n",
    "**Disadvantages of Bag-of-Words representation:**\n",
    "\n",
    "1) Lack of Semantic Understanding: BoW ignores the semantic meaning and context of words. It treats each word as an independent feature, disregarding the relationships and dependencies between words. This can limit the ability to capture the true meaning and nuances in text.\n",
    "\n",
    "2) Vocabulary Size: BoW representation typically results in a large vocabulary size, especially in large text collections. This can lead to high-dimensional feature vectors and increased memory and computational requirements.\n",
    "\n",
    "3) Loss of Word Order: BoW disregards the order of words in a sentence or document. This loss of word order information can be disadvantageous for tasks where word sequence and syntax play a crucial role, such as language generation or sentiment analysis where negations or modifiers affect the meaning.\n",
    "\n",
    "4) Sparsity: BoW representations tend to be sparse, especially when working with large vocabularies. Most documents only contain a small subset of the entire vocabulary, resulting in many zero values in the feature vectors. This sparsity can impact the efficiency and performance of machine learning algorithms.\n",
    "\n",
    "5) Ignoring Rare Words or Out-of-Vocabulary Words: BoW may not handle rare words or out-of-vocabulary words well. If a word does not appear frequently in the training data or is not part of the initial vocabulary, it may be ignored or treated as an unknown word, potentially losing important information.\n",
    "\n",
    "To address some of these limitations, various techniques, such as TF-IDF, word embeddings, and deep learning models like BERT, have been developed to capture more nuanced language patterns and semantic relationships between words.\n",
    "\n",
    "While BoW has its drawbacks, it still serves as a useful baseline representation and can be combined with other techniques to enhance the performance of NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd54e87",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80407eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b712e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is the best of times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iT is the worst of time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 90 was the AgE of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was the age of ^&amp; foolishness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text\n",
       "0           It is the best of times\n",
       "1           iT is the worst of time\n",
       "2       IT 90 was the AgE of wisdom\n",
       "3  It was the age of ^& foolishness"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"It is the best of times\",\n",
    "        \"iT is the worst of time\",\n",
    "        \"IT 90 was the AgE of wisdom\",\n",
    "        \"It was the age of ^& foolishness\"]\n",
    "\n",
    "df = pd.DataFrame({'text': text})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2931d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT 90 was the AgE of wisdom\n"
     ]
    }
   ],
   "source": [
    "raw = \"IT 90 was the AgE of wisdom\"\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2e038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT  was the AgE of wisdom\n"
     ]
    }
   ],
   "source": [
    "# re.sub is used to substitute (replace) any character that is not an alphabetic\n",
    "# letter or whitespace with an empty string\n",
    "sentence = re.sub('[^a-zA-Z\\s]', '', raw)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eff1c4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it  was the age of wisdom'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentence.lower()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92d74ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'the', 'age', 'of', 'wisdom']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "tokens = sentence.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a4bcdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'wisdom']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the stop words\n",
    "victim = set([\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\",\n",
    "    'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn',\n",
    "    \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"ff\", \"suffering\",\n",
    "    \"I\", \"and\"\n",
    "])\n",
    "\n",
    "clean_token = [i for i in tokens if not i in victim]\n",
    "clean_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e238d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3952934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10061] No\n",
      "[nltk_data]     connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532a9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# clean_tokens_lem = [lemmatizer.lemmatize(i) for i in clean_token]\n",
    "# clean_tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fde230f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c5f1528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'wisdom']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens_stem = [ps.stem(i) for i in clean_token]\n",
    "clean_tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e31aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vectorization UI\n",
    "def preprocess(a):\n",
    "    # Removing special characters and digits\n",
    "    sentence = re.sub(\"[^a-zA-Z\\s]\", \" \", a)\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # remove stop words                \n",
    "    clean_tokens = [t for t in tokens if t not in victim]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    \n",
    "    clean_tokens = [ps.stem(word) for word in clean_tokens]\n",
    "    \n",
    "    return pd.Series([\" \".join(clean_tokens), len(clean_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0d37a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1\n",
       "0    best time  2\n",
       "1   worst time  2\n",
       "2   age wisdom  2\n",
       "3  age foolish  2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the preprocessing to whole dataset\n",
    "text_df = df[\"text\"].apply(lambda x: preprocess(x))\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b42a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem text</th>\n",
       "      <th>stem text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stem text  stem text length\n",
       "0    best time                 2\n",
       "1   worst time                 2\n",
       "2   age wisdom                 2\n",
       "3  age foolish                 2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.columns = [\"stem text\", \"stem text length\"]\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92a81162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stem text</th>\n",
       "      <th>stem text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is the best of times</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iT is the worst of time</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 90 was the AgE of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was the age of ^&amp; foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text    stem text  stem text length\n",
       "0           It is the best of times    best time                 2\n",
       "1           iT is the worst of time   worst time                 2\n",
       "2       IT 90 was the AgE of wisdom   age wisdom                 2\n",
       "3  It was the age of ^& foolishness  age foolish                 2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, text_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285bf6f",
   "metadata": {},
   "source": [
    "### BOW Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbdaec",
   "metadata": {},
   "source": [
    "- **For bag of words we use the Countvectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcd03155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aeebce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cv.fit_transform(df[\"stem text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccaad66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c0d915e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24b77f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'best', 'foolish', 'time', 'wisdom', 'worst']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1e015ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de1462fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RITWIK GANGULY\\anaconda3\\envs\\Ritwik_DATA_Sci\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolish  time  wisdom  worst\n",
       "0    0     1        0     1       0      0\n",
       "1    0     0        0     1       0      1\n",
       "2    1     0        0     0       1      0\n",
       "3    1     0        1     0       0      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660ecc1",
   "metadata": {},
   "source": [
    "### n-gram approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ebed0",
   "metadata": {},
   "source": [
    "> In the BOW it can be possible, there we can not differentiate two documents with the single vocabulary, then for less no of documents and features_name we will use the pair of vocabulary to uniquely differentiate, the pair is done with 'n' no of pair, is called n-gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73fca39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocub = CountVectorizer(ngram_range=[1, 2])\n",
    "dtm2 = vocub.fit_transform(df[\"stem text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b246bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b6a70f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RITWIK GANGULY\\anaconda3\\envs\\Ritwik_DATA_Sci\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age foolish</th>\n",
       "      <th>age wisdom</th>\n",
       "      <th>best</th>\n",
       "      <th>best time</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "      <th>worst time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  age foolish  age wisdom  best  best time  foolish  time  wisdom  \\\n",
       "0    0            0           0     1          1        0     1       0   \n",
       "1    0            0           0     0          0        0     1       0   \n",
       "2    1            0           1     0          0        0     0       1   \n",
       "3    1            1           0     0          0        1     0       0   \n",
       "\n",
       "   worst  worst time  \n",
       "0      0           0  \n",
       "1      1           1  \n",
       "2      0           0  \n",
       "3      0           0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm2.toarray(), columns=vocub.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9ff9f",
   "metadata": {},
   "source": [
    "### TF-IDF Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fa9f8",
   "metadata": {},
   "source": [
    "> **TF-IDF (Term Frequency-Inverse Document Frequency) is a widely used technique in natural language processing (NLP) to quantify the importance of a word in a document within a collection or corpus of documents. TF-IDF combines two metrics: term frequency (TF) and inverse document frequency (IDF).**\n",
    "\n",
    "Here's a step-by-step explanation of the TF-IDF technique:\n",
    "\n",
    "- **Term Frequency (TF):** The term frequency measures the frequency of a word within a document. It is calculated by dividing the number of times a word appears in a document by the total number of words in that document. The assumption is that the more frequently a word appears in a document, the more important it is to that document.\n",
    "\n",
    "- **Inverse Document Frequency (IDF):** The inverse document frequency measures the rarity or importance of a word in the entire collection of documents. It is calculated by taking the logarithm of the ratio between the total number of documents and the number of documents containing the word. The IDF assigns higher weights to words that are less frequent in the overall document collection.\n",
    "\n",
    "- **TF-IDF Calculation:** The TF-IDF value for a word in a specific document is obtained by multiplying its term frequency (TF) in that document by its inverse document frequency (IDF) across the entire corpus. The idea is that words that appear frequently within a document but are relatively rare in the overall corpus tend to have higher TF-IDF values and are considered more significant to that particular document.\n",
    "\n",
    "- **Vectorization:** Once TF-IDF values are calculated for each word in the documents, they can be used to represent the documents as vectors. Each document is represented as a vector, where the dimensions correspond to the words in the vocabulary, and the values represent the TF-IDF scores for those words in the document. The TF-IDF vectorization captures the importance of words in each document while taking into account their rarity in the corpus.\n",
    "\n",
    "The TF-IDF technique helps address the limitations of simple word count-based representations like the Bag-of-Words (BoW) model. By considering both the local term frequency and the global document frequency, TF-IDF enables the identification of important words that carry specific information and discriminative power across the collection of documents.\n",
    "\n",
    "Applications of TF-IDF include information retrieval, document classification, text mining, and keyword extraction. It helps prioritize relevant documents based on their content and extract key terms that are indicative of the content's focus.\n",
    "\n",
    "Note that there are variations and refinements to the basic TF-IDF technique, such as adding smoothing factors or normalization to account for document length. These variations can be implemented based on specific requirements or to enhance the performance of TF-IDF in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbfe32",
   "metadata": {},
   "source": [
    "The TF-IDF (Term Frequency-Inverse Document Frequency) approach in natural language processing (NLP) offers several advantages and disadvantages. Let's explore them:\n",
    "\n",
    "**Advantages of TF-IDF approach:**\n",
    "\n",
    "1) Term Importance: TF-IDF assigns higher weights to terms that are important in a document while considering their rarity in the entire document collection. It helps to highlight terms that are discriminative and carry more informational value.\n",
    "\n",
    "2) Contextual Understanding: TF-IDF takes into account both the term frequency within a document (TF) and the inverse document frequency (IDF) across the entire collection. This allows it to capture the importance of a term within its specific context while considering its global relevance.\n",
    "\n",
    "3) Flexibility: TF-IDF can be adapted and customized to suit different requirements. It provides flexibility in terms of adjusting the IDF calculation, applying smoothing techniques, or incorporating additional features to enhance the representation.\n",
    "\n",
    "4) Language Independence: TF-IDF is language-independent and can be applied to various languages and domains. It focuses on the statistical properties of terms within a document collection rather than specific linguistic rules or patterns.\n",
    "\n",
    "5) Interpretable Representation: TF-IDF provides interpretable feature representations. The resulting TF-IDF vectors can be examined to understand the importance of different terms within each document, aiding in the analysis and interpretation of text data.\n",
    "\n",
    "Disadvantages of TF-IDF approach:\n",
    "\n",
    "1) Lack of Semantic Understanding: Similar to the Bag-of-Words (BoW) approach, TF-IDF does not capture the semantic meaning and context of words. It treats words as isolated entities and does not consider their relationships or order within a document.\n",
    "\n",
    "2) Term Ambiguity: TF-IDF treats each occurrence of a term as equally important within a document. This can lead to challenges in scenarios where terms have multiple meanings or ambiguities, as TF-IDF does not differentiate between them.\n",
    "\n",
    "3) Handling Out-of-Vocabulary Words: TF-IDF relies on a pre-defined vocabulary to calculate IDF scores. Out-of-vocabulary words that are not present in the vocabulary might be assigned zero IDF scores, losing their potential importance.\n",
    "\n",
    "4) Document Length Bias: Longer documents tend to have higher term frequencies. This can result in biased representations where longer documents dominate shorter ones, potentially affecting the performance of certain applications.\n",
    "\n",
    "5) Scaling with Vocabulary Size: As the vocabulary size increases, the computational and memory requirements of TF-IDF can grow significantly. Large vocabularies can lead to high-dimensional feature vectors and increased processing overhead.\n",
    "\n",
    "It's important to consider these advantages and disadvantages when applying TF-IDF in NLP tasks. TF-IDF is commonly used for information retrieval, text classification, and content-based recommendation systems, among other applications. However, for more advanced tasks that require capturing semantic meaning or word relationships, other techniques like word embeddings or pre-trained language models may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bfbedc",
   "metadata": {},
   "source": [
    "### WordToVec(W2V) Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9bb14",
   "metadata": {},
   "source": [
    "> The Word2Vec approach is a popular technique for learning word embeddings, which are vector representations of words in a continuous vector space. Word2Vec captures the semantic and syntactic relationships between words by mapping them to vectors in a way that similar words are closer together in the vector space. The approach is based on the distributional hypothesis, which suggests that words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "**There are two main variants of Word2Vec: Continuous Bag-of-Words (CBOW) and Skip-gram.**\n",
    "\n",
    "**Continuous Bag-of-Words (CBOW):**\n",
    "In CBOW, the model predicts the target word based on the context words surrounding it. The context words are treated as input features, and the target word is the output. The model learns to predict the target word given the context words. This process helps capture the distributional properties of words in a context. CBOW is computationally efficient and works well when the training data has a large amount of context.\n",
    "\n",
    "**Skip-gram:**\n",
    "In Skip-gram, the model predicts the context words given a target word. The target word is treated as the input, and the context words are the outputs. The model learns to represent a word in a way that it can predict the surrounding words in a sentence or document. Skip-gram is effective in capturing the meaning and usage of a word in different contexts. It performs better when the training data is limited or when rare words need to be accurately represented.\n",
    "\n",
    "The training process of Word2Vec involves updating the word vectors to minimize the loss between the predicted and actual words. The vectors are adjusted through an optimization algorithm, such as stochastic gradient descent (SGD). The objective is to learn word embeddings that capture the semantic relationships between words and allow for tasks such as word analogy and similarity calculations.\n",
    "\n",
    "Once trained, the learned word embeddings can be used for various NLP tasks. Similar words are represented by vectors that are close together in the vector space. Word analogy tasks can be performed by vector arithmetic, such as subtracting the vector of one word from another and finding the closest vector to the result. For example, \"king - man + woman\" would result in a vector close to the vector representation of \"queen.\"\n",
    "\n",
    "Word2Vec has gained popularity due to its ability to capture meaningful representations of words in a continuous vector space. These word embeddings can then be used as features in downstream NLP tasks such as text classification, named entity recognition, sentiment analysis, and machine translation.\n",
    "\n",
    "It's worth noting that Word2Vec is a static embedding technique, meaning it does not capture the contextual information or word meaning changes over time or different contexts. More recent models like GloVe, ELMo, and BERT have addressed this limitation by introducing contextualized word embeddings that consider the surrounding context when representing words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910c7aa",
   "metadata": {},
   "source": [
    "**Advantages of Word2Vec:**\n",
    "\n",
    "1) Semantic Similarity: Word2Vec captures semantic relationships between words. Words with similar meanings tend to have similar vector representations, allowing for tasks like word analogy and word similarity calculations.\n",
    "\n",
    "2) Dimensionality Reduction: Word2Vec reduces the high-dimensional one-hot encoding representation of words to a lower-dimensional dense vector space. This compression conserves memory and computational resources, making it easier to process and analyze text data.\n",
    "\n",
    "3) Out-of-Vocabulary (OOV) Handling: Word2Vec can handle out-of-vocabulary words to some extent by inferring their vector representations based on the context of surrounding words. This is useful when encountering words not seen during the training phase.\n",
    "\n",
    "4) Contextual Understanding: Word2Vec captures the contextual information of words. It learns from the co-occurrence patterns of words and their contexts, allowing it to understand the meaning and usage of words in different contexts.\n",
    "\n",
    "**Disadvantages of Word2Vec:**\n",
    "\n",
    "1) Contextual Limitations: Word2Vec typically considers a fixed context window size. Words outside this window may have less influence on the learned representations, which can limit its ability to capture long-range dependencies or contextual nuances.\n",
    "\n",
    "2) Rare Word Representations: Word2Vec may struggle to learn accurate representations for rare words with limited occurrences in the training data. Since rare words have fewer instances to learn from, their representations may be less reliable.\n",
    "\n",
    "3) Polysemy Challenges: Word2Vec may face challenges with words that have multiple meanings (polysemy) as it provides a single vector representation for each word, regardless of its context. This can lead to ambiguity in representing such words.\n",
    "\n",
    "4) Lack of Subword Information: Word2Vec treats words as atomic units and does not explicitly capture subword information. This can make it difficult to represent morphologically rich languages, where the meaning of words can be influenced by prefixes, suffixes, and root words.\n",
    "\n",
    "5) Limited to Static Representations: Word2Vec provides static word embeddings that do not capture word meaning changes over time or different contexts. It does not incorporate contextualized information, which is essential for certain NLP tasks.\n",
    "\n",
    "Despite these limitations, Word2Vec has been widely adopted and proved useful in various NLP applications, including word similarity, sentiment analysis, named entity recognition, and machine translation. It serves as a foundational technique in the field of word embeddings and has paved the way for more advanced models like GloVe and contextualized word embeddings such as ELMo and BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd2c4d",
   "metadata": {},
   "source": [
    "### BERT Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b3c6e",
   "metadata": {},
   "source": [
    "> BERT (Bidirectional Encoder Representations from Transformers) is a powerful technique in natural language processing (NLP) that revolutionized the field by introducing contextualized word embeddings. It was introduced by researchers at Google in 2018. BERT is based on the Transformer architecture, which is a type of neural network architecture that excels in capturing long-range dependencies in sequential data.\n",
    "\n",
    "BERT differs from previous approaches by pre-training a deep bidirectional transformer model on a large amount of unlabeled text data and then fine-tuning it on specific downstream NLP tasks. This pre-training and fine-tuning process allows BERT to learn rich representations of words that capture their contextual meaning and relationships.\n",
    "\n",
    "Here's an overview of the BERT technique:\n",
    "\n",
    "**Pre-training:**\n",
    "During the pre-training phase, BERT is trained on a large corpus of unlabeled text data, such as Wikipedia. The model learns to predict masked words in sentences by considering both the left and right context of each word. It also learns to predict whether two sentences follow each other in the original text. This bidirectional training allows BERT to capture the meaning of words based on their context.\n",
    "\n",
    "**Fine-tuning:**\n",
    "After pre-training, BERT is fine-tuned on specific downstream NLP tasks, such as text classification, named entity recognition, question answering, or sentiment analysis. The model is further trained on task-specific labeled data to adapt its learned representations to the target task. This fine-tuning process allows BERT to specialize in the specific semantics and nuances of the task at hand.\n",
    "\n",
    "**Advantages of BERT:**\n",
    "\n",
    "**Contextualized Word Representations:** BERT captures the contextual meaning of words by considering their surrounding context. It produces dynamic word embeddings that change based on the context they appear in, allowing for a better understanding of word meanings and disambiguation.\n",
    "\n",
    "**State-of-the-Art Performance:** BERT has achieved state-of-the-art performance on various NLP benchmarks and tasks, surpassing previous models in tasks such as question answering, named entity recognition, sentiment analysis, and text classification.\n",
    "\n",
    "**Language Understanding:** BERT has been pre-trained on large-scale corpora, allowing it to gain a broad understanding of language patterns and semantics. It can handle a wide range of languages and tasks, making it highly versatile.\n",
    "\n",
    "**Transfer Learning:** BERT's pre-training and fine-tuning approach enables transfer learning. The pre-trained model can be fine-tuned on specific tasks with relatively smaller labeled datasets, saving time and resources.\n",
    "\n",
    "**Disadvantages of BERT:**\n",
    "\n",
    "**Computational Requirements:** BERT is a large and complex model, consisting of several layers and a vast number of parameters. Training and using BERT can require substantial computational resources, including powerful hardware and significant memory.\n",
    "\n",
    "**Training Data Dependency:** BERT's performance heavily relies on the quality and representation of the pre-training data. If the pre-training corpus does not cover the target domain or lacks diversity, the fine-tuned model may not generalize well to specific tasks.\n",
    "\n",
    "**Lack of Interpretability:** BERT's representations are complex and not easily interpretable by humans. Understanding the inner workings of the model and how it arrives at its decisions can be challenging.\n",
    "\n",
    "BERT has significantly advanced the field of NLP by providing highly effective contextualized word embeddings. Its ability to capture rich semantic relationships and contextual understanding has made it a fundamental component in various NLP applications and has paved the way for subsequent models that build upon its success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429cea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
